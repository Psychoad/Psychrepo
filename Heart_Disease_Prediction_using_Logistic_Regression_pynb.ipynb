{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBAH8faE4IJO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def mount_google_drive(drive_path='/content/drive'):\n",
        "    \"\"\"\n",
        "    Mounts Google Drive and verifies the dataset file exists.\n",
        "\n",
        "    Parameters:\n",
        "        drive_path (str): Path where Google Drive will be mounted (default: /content/drive)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if mounted successfully and dataset exists, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Mount Google Drive\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount(drive_path, force_remount=True)\n",
        "\n",
        "        # Verify mount\n",
        "        if os.path.exists(drive_path):\n",
        "            print(f\"Google Drive mounted successfully at {drive_path}\")\n",
        "\n",
        "            # Example: Check if your dataset exists (replace with your dataset path)\n",
        "            dataset_path = os.path.join(drive_path, 'MyDrive/your_dataset.csv')  # Adjust path\n",
        "            if os.path.exists(dataset_path):\n",
        "                print(f\"Dataset found at {dataset_path}\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Error: Dataset not found at {dataset_path}\")\n",
        "                return False\n",
        "        else:\n",
        "            print(f\"Error: Drive mount path {drive_path} does not exist\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Google Drive mounting: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def list_drive_contents(drive_path='/content/drive/MyDrive', max_files=10):\n",
        "    \"\"\"\n",
        "    Lists contents of a directory in Google Drive to help locate your dataset.\n",
        "\n",
        "    Parameters:\n",
        "        drive_path (str): Path to list contents from\n",
        "        max_files (int): Maximum number of files/folders to display\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if os.path.exists(drive_path):\n",
        "            print(f\"\\nListing contents of {drive_path}:\")\n",
        "            files = os.listdir(drive_path)[:max_files]\n",
        "            for file in files:\n",
        "                print(f\" - {file}\")\n",
        "            if len(files) == 0:\n",
        "                print(\"Directory is empty\")\n",
        "        else:\n",
        "            print(f\"Error: Directory {drive_path} does not exist\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing directory contents: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    success = mount_google_drive()\n",
        "\n",
        "    if success:\n",
        "        # List contents of MyDrive to help locate dataset\n",
        "        list_drive_contents()\n",
        "    else:\n",
        "        print(\"Failed to mount Google Drive or locate dataset. Please check your setup.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TLA_N5S04xGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "shB2bfop5GfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def clean_data(file_path, output_path='cleaned_dataset.csv'):\n",
        "    try:\n",
        "        # Load dataset\n",
        "        df = pd.read_csv('/content/drive/MyDrive/archive.zip')\n",
        "\n",
        "        # Check if dataset is empty\n",
        "        if df.empty:\n",
        "            raise ValueError(\"Dataset is empty\")\n",
        "\n",
        "        # Display initial info\n",
        "        print(\"Initial Dataset Info:\")\n",
        "        print(df.info())\n",
        "        print(\"\\nInitial Dataset Description:\")\n",
        "        print(df.describe())\n",
        "\n",
        "        # Handle missing values\n",
        "        if df.isnull().sum().any():\n",
        "            print(\"Missing values detected. Filling with median for numerical columns...\")\n",
        "            df = df.fillna(df.median(numeric_only=True))\n",
        "\n",
        "        # Remove duplicates\n",
        "        duplicates = df.duplicated().sum()\n",
        "        if duplicates > 0:\n",
        "            print(f\"Removing {duplicates} duplicate rows...\")\n",
        "            df = df.drop_duplicates()\n",
        "\n",
        "        # Convert categorical variables to numeric using one-hot encoding\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "        if len(categorical_cols) > 0:\n",
        "            print(\"Encoding categorical columns...\")\n",
        "            df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "        # Ensure all data is numeric\n",
        "        df = df.astype(float)\n",
        "\n",
        "        # Handle infinite values\n",
        "        if np.any(np.isinf(df)):\n",
        "            print(\"Infinite values detected. Replacing with column mean...\")\n",
        "            df = df.replace([np.inf, -np.inf], np.nan).fillna(df.mean())\n",
        "\n",
        "        # Check if target column exists\n",
        "        if 'target' not in df.columns:\n",
        "            raise ValueError(\"Target column 'target' not found in dataset\")\n",
        "\n",
        "        # Save cleaned dataset\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"\\nData Cleaning Completed! Cleaned dataset saved to {output_path}\")\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data cleaning: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = 'your_dataset.csv'  # Replace with your dataset file path\n",
        "    clean_data(file_path)"
      ],
      "metadata": {
        "id": "se4AOMow4zdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Normalization"
      ],
      "metadata": {
        "id": "NThUctX15JVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def normalize_data(file_path, output_path='normalized_dataset.csv'):\n",
        "    try:\n",
        "        # Load cleaned dataset\n",
        "        df = pd.read_csv('/content/drive/MyDrive/archive.zip')\n",
        "\n",
        "        # Check if dataset is empty\n",
        "        if df.empty:\n",
        "            raise ValueError(\"Dataset is empty\")\n",
        "\n",
        "        # Check if target column exists\n",
        "        if 'target' not in df.columns:\n",
        "            raise ValueError(\"Target column 'target' not found in dataset\")\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df.drop('target', axis=1)\n",
        "        y = df['target']\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "        # Combine scaled features with target\n",
        "        df_normalized = pd.concat([X_scaled, y.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        # Save normalized dataset\n",
        "        df_normalized.to_csv(output_path, index=False)\n",
        "        print(f\"\\nData Normalization Completed! Normalized dataset saved to {output_path}\")\n",
        "        return df_normalized, scaler\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data normalization: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = 'cleaned_dataset.csv'  # Replace with your cleaned dataset file path\n",
        "    normalize_data(file_path)"
      ],
      "metadata": {
        "id": "aUtLA5A45A2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Visualization"
      ],
      "metadata": {
        "id": "3q6kU1mC5XmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def extract_zip(zip_path, extract_path='/content/extracted_data'):\n",
        "    \"\"\"\n",
        "    Extracts a zip file to the specified directory and returns the path to the first .csv file found.\n",
        "\n",
        "    Parameters:\n",
        "        zip_path (str): Path to the zip file\n",
        "        extract_path (str): Directory to extract files to\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the extracted .csv file, or None if not found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(extract_path, exist_ok=True)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(f\"Extracted zip file to {extract_path}\")\n",
        "\n",
        "        # Find the first .csv file in the extracted directory\n",
        "        for file in os.listdir(extract_path):\n",
        "            if file.endswith('.csv'):\n",
        "                return os.path.join(extract_path, file)\n",
        "        print(\"Error: No .csv file found in the zip archive\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting zip file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def visualize_data(file_path, save_dir='/content/drive/MyDrive/HeartDiseasePlots'):\n",
        "    \"\"\"\n",
        "    Visualizes the heart disease dataset with various plots and displays them interactively.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the dataset (.csv or .zip)\n",
        "        save_dir (str): Directory to save plots\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if file is a zip\n",
        "        if file_path.endswith('.zip'):\n",
        "            file_path = extract_zip(file_path)\n",
        "            if file_path is None:\n",
        "                return\n",
        "\n",
        "        # Load dataset\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if dataset is empty\n",
        "        if df.empty:\n",
        "            raise ValueError(\"Dataset is empty\")\n",
        "\n",
        "        # Check if target column exists\n",
        "        if 'target' not in df.columns:\n",
        "            raise ValueError(\"Target column 'target' not found in dataset\")\n",
        "\n",
        "        # Create directory for saving plots\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        print(f\"Saving plots to {save_dir}\")\n",
        "\n",
        "        # 1. Correlation Heatmap\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "        plt.title('Correlation Heatmap of Heart Disease Features')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'correlation_heatmap.png'))\n",
        "        plt.show()  # Display plot\n",
        "        print(\"Displayed and saved correlation_heatmap.png\")\n",
        "\n",
        "        # 2. Pairplot for selected features (limit to 5 features + target)\n",
        "        selected_cols = df.columns[:5].tolist() + ['target']\n",
        "        sns.pairplot(df[selected_cols], hue='target', palette='coolwarm')\n",
        "        plt.suptitle('Pairplot of Selected Features (Colored by Target)', y=1.02)\n",
        "        plt.savefig(os.path.join(save_dir, 'pairplot.png'))\n",
        "        plt.show()  # Display plot\n",
        "        print(\"Displayed and saved pairplot.png\")\n",
        "\n",
        "        # 3. Distribution of Target\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(df['target'], kde=True, color='purple')\n",
        "        plt.title('Distribution of Target (Heart Disease Risk)')\n",
        "        plt.xlabel('Target Value')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'target_distribution.png'))\n",
        "        plt.show()  # Display plot\n",
        "        print(\"Displayed and saved target_distribution.png\")\n",
        "\n",
        "        # 4. Box Plot for Key Features by Target\n",
        "        key_features = ['age', 'chol', 'thalach']  # Adjust based on your dataset\n",
        "        valid_features = [col for col in key_features if col in df.columns]\n",
        "        if valid_features:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for i, feature in enumerate(valid_features, 1):\n",
        "                plt.subplot(1, len(valid_features), i)\n",
        "                sns.boxplot(x='target', y=feature, data=df, palette='coolwarm')\n",
        "                plt.title(f'{feature} by Target')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, 'boxplot_features.png'))\n",
        "            plt.show()  # Display plot\n",
        "            print(\"Displayed and saved boxplot_features.png\")\n",
        "\n",
        "        # 5. Scatter Plot for Two Key Features\n",
        "        if 'age' in df.columns and 'chol' in df.columns:\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.scatterplot(x='age', y='chol', hue='target', size='target', data=df, palette='coolwarm')\n",
        "            plt.title('Age vs Cholesterol (Colored by Target)')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, 'scatter_age_chol.png'))\n",
        "            plt.show()  # Display plot\n",
        "            print(\"Displayed and saved scatter_age_chol.png\")\n",
        "\n",
        "        print(\"\\nData Visualization Completed!\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data visualization: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the path to your zip file or csv in Google Drive\n",
        "    file_path = '/content/drive/MyDrive/archive.zip'  # Adjust to your file path\n",
        "    visualize_data(file_path)"
      ],
      "metadata": {
        "id": "QCG3kNtY6arJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction"
      ],
      "metadata": {
        "id": "DpmLuyGf6qhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def extract_features(file_path, output_path='selected_features_dataset.csv', threshold=0.1):\n",
        "    \"\"\"\n",
        "    Extracts features based on correlation with the target variable.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the input dataset (e.g., normalized dataset)\n",
        "        output_path (str): Path to save the dataset with selected features\n",
        "        threshold (float): Minimum absolute correlation threshold for feature selection\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Dataset with selected features and target, or None if error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load normalized dataset\n",
        "        print(f\"Loading dataset from {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if dataset is empty\n",
        "        if df.empty:\n",
        "            raise ValueError(\"Dataset is empty\")\n",
        "\n",
        "        # Display basic info for debugging\n",
        "        print(\"Dataset Info:\")\n",
        "        print(df.info())\n",
        "\n",
        "        # Check if target column exists\n",
        "        if 'target' not in df.columns:\n",
        "            raise ValueError(\"Target column 'target' not found in dataset. Available columns: \" + str(df.columns.tolist()))\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df.drop('target', axis=1)\n",
        "        y = df['target']\n",
        "\n",
        "        # Ensure all features are numeric\n",
        "        non_numeric_cols = X.select_dtypes(exclude=['float64', 'int64']).columns\n",
        "        if len(non_numeric_cols) > 0:\n",
        "            print(f\"Warning: Non-numeric columns detected: {non_numeric_cols}. Converting to numeric where possible...\")\n",
        "            for col in non_numeric_cols:\n",
        "                try:\n",
        "                    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "                except:\n",
        "                    raise ValueError(f\"Cannot convert column {col} to numeric\")\n",
        "            # Fill any NaNs from conversion with median\n",
        "            X = X.fillna(X.median(numeric_only=True))\n",
        "\n",
        "        # Check for infinite values\n",
        "        if np.any(np.isinf(X)):\n",
        "            print(\"Infinite values detected. Replacing with column mean...\")\n",
        "            X = X.replace([np.inf, -np.inf], np.nan).fillna(X.mean(numeric_only=True))\n",
        "\n",
        "        # Calculate correlation with target\n",
        "        print(\"Calculating correlations...\")\n",
        "        corr = pd.concat([X, y], axis=1).corr()['target'].abs().sort_values(ascending=False)\n",
        "        corr = corr.drop('target', errors='ignore')  # Remove self-correlation\n",
        "\n",
        "        # Select features above threshold\n",
        "        selected_features = corr[corr > threshold].index.tolist()\n",
        "        if not selected_features:\n",
        "            raise ValueError(f\"No features have correlation above threshold {threshold}. Try lowering the threshold.\")\n",
        "\n",
        "        # Create dataset with selected features\n",
        "        X_selected = X[selected_features]\n",
        "        df_selected = pd.concat([X_selected, y.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        # Save dataset with selected features\n",
        "        df_selected.to_csv(output_path, index=False)\n",
        "        print(f\"\\nSelected Features: {selected_features}\")\n",
        "        print(f\"Feature Extraction Completed! Dataset saved to {output_path}\")\n",
        "        return df_selected\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found. Please verify the path.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during feature extraction: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/content/extracted_data/heart.csv'  # Path to your dataset\n",
        "    extract_features(file_path)"
      ],
      "metadata": {
        "id": "e_LmUDAO6kUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training and Evaluation using Logistic Regression"
      ],
      "metadata": {
        "id": "3ZsxKoic8Suh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def train_and_evaluate_model(file_path):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a Logistic Regression model for heart disease prediction.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the dataset with selected features\n",
        "\n",
        "    Returns:\n",
        "        LogisticRegression: Trained model, or None if error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load dataset with selected features\n",
        "        print(f\"Loading dataset from {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if dataset is empty\n",
        "        if df.empty:\n",
        "            raise ValueError(\"Dataset is empty\")\n",
        "\n",
        "        # Check if target column exists\n",
        "        if 'target' not in df.columns:\n",
        "            raise ValueError(\"Target column 'target' not found in dataset. Available columns: \" + str(df.columns.tolist()))\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df.drop('target', axis=1)\n",
        "        y = df['target']\n",
        "\n",
        "        # Ensure target is binary (0 or 1)\n",
        "        if not y.isin([0, 1]).all():\n",
        "            raise ValueError(\"Target column must contain binary values (0 or 1)\")\n",
        "\n",
        "        # Ensure all features are numeric\n",
        "        non_numeric_cols = X.select_dtypes(exclude=['float64', 'int64']).columns\n",
        "        if len(non_numeric_cols) > 0:\n",
        "            print(f\"Warning: Non-numeric columns detected: {non_numeric_cols}. Converting to numeric...\")\n",
        "            for col in non_numeric_cols:\n",
        "                try:\n",
        "                    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "                except:\n",
        "                    raise ValueError(f\"Cannot convert column {col} to numeric\")\n",
        "            X = X.fillna(X.median(numeric_only=True))\n",
        "\n",
        "        # Check for infinite values\n",
        "        if np.any(np.isinf(X)):\n",
        "            print(\"Infinite values detected. Replacing with column mean...\")\n",
        "            X = X.replace([np.inf, -np.inf], np.nan).fillna(X.mean(numeric_only=True))\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Train Logistic Regression model\n",
        "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluate model\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "        # Feature importance (coefficients)\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': X.columns,\n",
        "            'Coefficient': model.coef_[0]\n",
        "        })\n",
        "        print(\"\\nFeature Importance (Logistic Regression Coefficients):\")\n",
        "        print(feature_importance.sort_values(by='Coefficient', ascending=False))\n",
        "\n",
        "        # Example: Predict on a new sample\n",
        "        sample = X.iloc[0].values.reshape(1, -1)\n",
        "        prediction = model.predict(sample)\n",
        "        prob = model.predict_proba(sample)[0]\n",
        "        print(f\"\\nSample Prediction (0 = No Disease, 1 = Disease): {prediction[0]}\")\n",
        "        print(f\"Prediction Probabilities (No Disease, Disease): {prob[0]:.4f}, {prob[1]:.4f}\")\n",
        "\n",
        "        print(\"\\nModel Training and Evaluation Completed!\")\n",
        "        return model\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model training/evaluation: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/content/selected_features_dataset.csv'  # Path to your selected features dataset\n",
        "    train_and_evaluate_model(file_path)"
      ],
      "metadata": {
        "id": "jkfa9HxA8SPw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}